head(afinn_darknet)
####move to AFINN lexicon
afinn <- get_sentiments("afinn") ##import the bing lexicon
head(afinn)
afinn_darknet<-merge(darknet_words, afinn,all.x=TRUE) ##match up entries
##take a peak
head(afinn_darknet)
###create numeric metrics
afinn_darknet$value[is.na(afinn_darknet$value)]<-0
##create negative sentiment measure
afinn_darknet$negative_sentiment<-0
afinn_darknet$negative_sentiment[afinn_darknet$value<0]<-afinn_darknet$value[afinn_darknet$value<0]
##create positive sentiment measure
afinn_darknet$positive_sentiment<-0
afinn_darknet$positive_sentiment[afinn_darknet$value>0]<-afinn_darknet$value[afinn_darknet$value>0]
###return to the transaction-level
negative_transactions<-aggregate(negative_sentiment~sales_review+Buyer+Seller+Date,FUN=sum,data=afinn_darknet)
positive_transactions<-aggregate(positive_sentiment~sales_review+Buyer+Seller+Date,FUN=sum,data=afinn_darknet)
all_transactions<-aggregate(value~sales_review+Buyer+Seller+Date,FUN=sum,data=afinn_darknet)
View(all_transactions)
View(all_transactions)
View(all_transactions)
View(all_transactions)
View(negative_transactions)
View(negative_transactions)
##combine datasets
all_transactions$negative_sentiment<-negative_transactions$negative_sentiment
all_transactions$positive_sentiment<-positive_transactions$positive_sentiment
colnames(all_transactions)[5]<-"net_tone"
range(all_transactions$positive_sentiment) ##some people very happy
range(all_transactions$negative_sentiment) ##some people very upset
##do we get the same outliers?
all_transactions[all_transactions$positive_sentiment==21,] #Nope!
total_transaction<-aggregate(value~book+word+book+linenumber,FUN=sum,data=afinn_darknet)
total_transaction<-aggregate(value~book+word+book+linenumber,FUN=sum,data=afinn_books)
##combine datasets
total_transaction$negative_sentiment<-neg_tran$negative_sentiment
##combine datasets
total_transaction$negative_sentiment<-neg_tran$negative_sentiment
###return to the transaction-level
neg_tran<-aggregate(negative_sentiment~book+word+book+linenumber, FUN=sum,data=afinn_books)
post_tran<-aggregate(positive_sentiment~book+word+book+linenumber,FUN=sum,data=afinn_books)
##combine datasets
total_transaction$negative_sentiment<-neg_tran$negative_sentiment
post_tran<-aggregate(positive_sentiment~book+word+book+linenumber,FUN=sum,data=afinn_books)
##combine datasets
total_transaction$negative_sentiment<-neg_tran$negative_sentiment
total_transaction$positive_sentiment<-post_tran$positive_sentiment
View(total_transaction)
View(total_transaction)
colnames(total_transaction)[4]<-"net_tone"
## AFINN sentiment analysis
mean(total_transaction$net_tone)
mean(total_transaction$positive_sentiment)
mean(total_transaction$negative_sentiment)
range(total_transaction$net_tone)
range(total_transaction$positive_sentiment)
range(total_transaction$negative_sentiment)
sd(total_transaction$net_tone)
sd(total_transaction$positive_sentiment)
sd(total_transaction$negative_sentiment)
hist(total_transaction$net_tone)
hist(total_transaction$net_tone)
hist(all_trans$net_tone)
sd(total_transaction$net_tone)
View(afinn_books)
View(total_transaction)
##now examine trust
NRC<-get_sentiments("nrc")
View(NRC)
##let's focus on a particular sentiment: trust
NRC<-NRC[NRC$sentiment=="trust",]
View(NRC)
NRC<-NRC[NRC$sentiment=="fear",]
View(NRC)
unique(NRC$sentiment)
nrc_darknet<-merge(darknet_words, NRC,all.x=TRUE) ##match up entries
View(nrc_darknet)
##take a peak
head(nrc_darknet)
###create numeric metrics
nrc_darknet$trust<-0
nrc_darknet$trust[nrc_darknet$sentiment=="trust"]<-1
nrc_transactions<-aggregate(trust~sales_review+Buyer+Seller+Date,FUN=sum,data=nrc_darknet)
hist(nrc_transactions$trust)
mean(nrc_transactions$trust) ##average transactions have .68
range(nrc_transactions$trust)
##who is the high-trust outlier?
nrc_transactions[nrc_transactions$trust==7,]
darknet_text[c(3946),] ##clearly very happy
##who is the high-trust outlier?
nrc_transactions[nrc_transactions$trust==7,]
##let's focus on a particular sentiment: trust
NRC<-NRC[NRC$sentiment=="trust",]
##now examine trust
NRC<-get_sentiments("nrc")
##let's focus on a particular sentiment: trust
NRC<-NRC[NRC$sentiment=="trust",]
unique(NRC$sentiment)
nrc_darknet<-merge(darknet_words, NRC,all.x=TRUE) ##match up entries
##take a peak
head(nrc_darknet)
###create numeric metrics
nrc_darknet$trust<-0
nrc_darknet$trust[nrc_darknet$sentiment=="trust"]<-1
nrc_transactions<-aggregate(trust~sales_review+Buyer+Seller+Date,FUN=sum,data=nrc_darknet)
hist(nrc_transactions$trust)
mean(nrc_transactions$trust) ##average transactions have .68
range(nrc_transactions$trust)
##who is the high-trust outlier?
nrc_transactions[nrc_transactions$trust==7,]
darknet_text[c(3946),] ##clearly very happy
darknet_text[c(2396),] ##clearly very happy
View(NRC)
View(NRC)
View(nrc_darknet)
View(nrc_darknet)
NRC<-get_sentiments("nrc")
View(NRC)
View(NRC)
NRC<-get_sentiments("nrc")
##let's focus on a particular sentiment: trust
NRC<-NRC[NRC$sentiment=="positive",]
unique(NRC$sentiment)
nrc_books<-merge(afinn_books, NRC,all.x=TRUE) ##match up entries
head(nrc_books)
View(nrc_books)
View(nrc_books)
nrc_trans<-aggregate(positive~book+word+book+linenumber,FUN=sum,data=nrc_books)
nrc_trans<-aggregate(positive~book+word+book+linenumber,FUN=sum,data=nrc_books)
View(nrc_books)
View(nrc_books)
nrc_books$positive<-0
nrc_books$positive[nrc_books$sentiment=="positive"]<-1
nrc_trans<-aggregate(positive~book+word+book+linenumber,FUN=sum,data=nrc_books)
hist(nrc_trans$positive)
mean(nrc_trans$positive) ##average transactions have .68
range(nrc_trans$positive)
sd(nrc_trans$positive)
View(nrc_trans)
View(nrc_trans)
View(nrc_books)
View(nrc_books)
View(nrc_trans)
View(nrc_trans)
nrc_trans[nrc_trans$positive==3,]
names(sort(table(austen_words$book)))
names(sort(table(nrc_trans$positive==3)))
names(sort(table(nrc_trans$positive)))
nrc_trans[nrc_trans$positive==3,]
sort(table(nrc_trans[nrc_trans$positive==3,]))
View(nrc_trans)
View(nrc_trans)
View(afinn_books)
View(afinn_books)
nrc_trans[nrc_trans$positive==1,]
View(NRC)
View(NRC)
NRC<-get_sentiments("nrc")
View(NRC)
View(NRC)
View(bing)
View(bing)
View(afinn)
View(afinn)
View(afinn_books)
View(afinn_books)
View(afinn_books)
View(afinn_books)
names(sort(table(austen_words$book)))
getwd()
## PART 2
load("Coronavirus_tweets.rdata")
View(covid_tweets)
View(covid_tweets)
## PART 2
load("Coronavirus_tweets.rdata")
head(covid_tweets)
covid_tweets[1:5]
covid_tweets[1:10]
View(covid_tweets)
View(covid_tweets)
covid_tweets$tweet_text[1:10]
setwd("C:/Users/alexs/Documents/") ##make sure to set your directory to the folder containing your data
load("Trump_Tweets.RData")
library(tidyverse)
setwd("C:/Users/alexs/Documents/plan372-sp23")
res=read_csv("restaurant_inspections.csv")
ggplot(res, aes(x=SCORE)) +
geom_histogram() +
xlab("Inspection Score") +
ggtitle("Restauraunt Inspection Scores") +
theme_minimal() #GGplot histogram with inspection scores
#Mean average of oldest half, mean average of younger half
res=res[order(res$RESTAURANTOPENDATE),] #orders the restaurant open date from oldest to newest
nrow(res)#count rows divide the data in half with 1938 for older restaurants and 1937 for newer restaurants
older=res[1:1938,] #create new data frame with oldest restauraunts
newer=res[1939:3875,] #create new data frame with newest restaurants
mean(older$SCORE) #mean score of 97.10
mean(newer$SCORE) #mean score of 96.74
#3
unique(res$CITY) #check how many city names, only 12 counties
res$newcity= str_to_upper(res$CITY) #create new column with upper case cities
res$newcity = recode(res$newcity,
"FUQUAY-VARINA" ="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA", "Fuquay Varina"="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA",
"RTP"= "RESEARCH TRIANGLE PARK",
"HOLLY SPRING"="HOLLY SPRINGS",
"MORRISVILE" = "MORRISVILLE",
) #recode Fuquay Varina, RTP, Holly Springs, and Morrisville
unique(res$newcity) #check to see if there are duplicates of cities
city_score_mean= group_by(res, newcity) %>% #groups the cities and finds the mean score of each city
summarize(average_score=mean(SCORE))
ordered_city_score=city_score_mean[order(-city_score_mean$average_score),] #orders the average inspections scores of each city
#4
unique(res$INSPECTOR) #count how many inspectors
#find the mean average of scores from inspectors
inspector_score=group_by(res,INSPECTOR)%>%
summarize(inspector_score_average=mean(SCORE))
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspector_scores=count(SCORE))
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspector_scores=nrow(SCORE))
View(inspector_scores)
View(inspector_scores)
View(inspector_score)
View(inspector_score)
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspector_scores1=nrow(SCORE))
View(inspector_scores)
View(inspector_scores)
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspectorscore=nrow(SCORE))
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspectorscore=mean(SCORE))
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspectorscore=sum(SCORE))
View(inspector_scores)
View(inspector_scores)
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspectorscore=count(SCORE))
grepl("clean",res$DESCRIPTION)
res$des1<-grepl("clean",res$DESCRIPTION)
View(res)
View(res)
res$des1<-is.na(res$DESCRIPTION)
View(res)
View(res)
res$empty_description<-is.na(res$DESCRIPTION)
#could also look at the number of description per inspector
inspector_descriptions=group_by(res, INSPECTOR)%>%
summarize(description_count=count(empty_description))
#could also look at the number of description per inspector
inspector_descriptions=group_by(res, INSPECTOR)%>%
summarize(description_count=sum(empty_description))
View(inspector_descriptions)
View(inspector_descriptions)
#could also look at the number of description per inspector
#find total percentage
inspector_descriptions=group_by(res, INSPECTOR)%>%
summarize(description_count=sum(!empty_description))
#could also look at the number of description per inspector
#find total percentage
inspector_descriptions=group_by(res, INSPECTOR)%>%
summarize(description_count=sum(empty_description)/(sum(empty_description)+sum(!empty_description)))
View(inspector_descriptions)
View(ordered_city_score)
View(ordered_city_score)
inspector_descriptions=inspector_descriptions[order(-inspector_descriptions$description_count)]
View(inspector_descriptions)
View(inspector_descriptions)
inspector_descriptions=inspector_descriptions[order(-inspector_descriptions$description_count),]
View(inspector_descriptions)
View(inspector_descriptions)
inspector_descriptions=inspector_descriptions[order(inspector_descriptions$description_count),]
library(tidyverse)
setwd("C:/Users/alexs/Documents/plan372-sp23")
res=read_csv("restaurant_inspections.csv")
#Mean average of oldest half, mean average of younger half
res=res[order(res$RESTAURANTOPENDATE),] #orders the restaurant open date from oldest to newest
#3
unique(res$CITY) #check how many city names, only 12 counties
res$newcity= str_to_upper(res$CITY) #create new column with upper case cities
res$newcity = recode(res$newcity,
"FUQUAY-VARINA" ="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA", "Fuquay Varina"="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA",
"RTP"= "RESEARCH TRIANGLE PARK",
"HOLLY SPRING"="HOLLY SPRINGS",
"MORRISVILE" = "MORRISVILLE",
) #recode Fuquay Varina, RTP, Holly Springs, and Morrisville
unique(res$newcity) #check to see if there are duplicates of cities
city_score_mean= group_by(res, newcity) %>% #groups the cities and finds the mean score of each city
summarize(average_score=mean(SCORE))
View(city_score_mean)
View(city_score_mean)
city_score_mean=city_score_mean[order(-city_score_mean$average_score),] #orders the average inspections scores of each city
View(city_score_mean)
View(city_score_mean)
#4
unique(res$INSPECTOR) #count how many inspectors
#find the mean average of scores from inspectors
inspector_score=group_by(res,INSPECTOR)%>%
summarize(inspector_score_average=mean(SCORE))
View(inspector_score)
View(inspector_score)
inspector_score=inspector_score[order(-inspector_score$inspector_score_average),] #order city average score from highest average given to lowest
inspector_scores=group_by(res,INSPECTOR)%>%
summarize(inspectorscore=count(SCORE))
#could also look at the number of description per inspector
res$empty_description<-is.na(res$DESCRIPTION)
inspector_descriptions=group_by(res, INSPECTOR)%>%
summarize(description_count=sum(empty_description)/(sum(empty_description)+sum(!empty_description))) #Divides number of empty descriptions from the total possible number of descriptions
inspector_descriptions=inspector_descriptions[order(inspector_descriptions$description_count),] #ranks the inspectors with the least descriptions with NA to most
View(city_score_mean)
View(city_score_mean)
View(inspector_descriptions)
View(res)
View(res)
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=nrow(SCORE))
View(city_inspections_count)
View(city_inspections_count)
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=sum(SCORE))
View(city_inspections_count)
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=sum(newcity))
uniqe(res$newcity)
unique(res$newcity)
names(sort(table(res$newcity)))
names(-sort(table(res$newcity)))
name(sort(table(res$newcity)))
-names(sort(table(res$newcity)))
names(sort(table(res$newcity)),)
de=names(sort(table(res$newcity)))
table1=table(res$CITY,res$SCORE)
table1
table1=table(res$newcity,res$SCORE)
table1
table1=table(res$newcity,res$DESCRIPTION)
table1
nrow(res$newcity=="RALEIGH")
count(res$newcity=="RALEIGH")
nrow(res$newcity=="RALEIGH")
sum(res$newcity=="RALEIGH")
res[res$newcity=="RALEIGH"]
res[res$newcity=="RALEIGH",]
count(res[res$newcity=="RALEIGH",])
count(res[res$newcity=="ANDIER",])
unique(res$newcity) #check to see if there are duplicates of cities
count(res[res$newcity=="ANGIER",])
count(res[res$newcity=="CLAYTON",])
count(res[res$newcity=="NORTH CAROLINA",])
count(res[res$newcity=="GARNER",])
count(res[res$newcity=="RALEIGH",])
count(res[res$newcity=="WILLOW S",])
count(res[res$newcity=="WILLOW SRING",])
count(res[res$newcity=="asd",])
tail(names(sort(table(res$newcity))), 1)
table(res$newcity, useNA="ifany")
tail(names(sort(table(res$newcity))), 1)
newtab=table(res$newcity, useNA="ifany")
data.frame.matrix(newtab)
as.data.frame.matrix(newtab)
data.frame(rbind(newtab))
a=data.frame(rbind(newtab))
View(a)
View(a)
View(a)
View(a)
t(a)
View(a)
View(a)
a=t(a)
newtab=table(res$newcity, useNA="ifany")
a=data.frame(rbind(newtab))
a=t(a)
newtab=table(res$newcity, useNA="ifany")
a=data.frame(rbind(newtab))
t(a)
asd=t(a)
asd=data.frame(t(a[-1]))
View(asd)
View(asd)
asd=asd[order(-asd$newtab),]
View(a)
View(a)
asd=data.frame(t(a[-1]))
asd[order(-asd$newtab),]
View(asd)
View(asd)
View(inspector_descriptions)
View(inspector_descriptions)
table1=(res$newcity)
str_count(res$newcity, "RALEIGH")
res$brah=str_count(res$newcity, "RALEIGH")
View(res)
View(res)
count(res$newcity="RALEIGH")
nrow(res$newcity="RALEIGH")
res$newcity="RALEIGH"
#3
unique(res$CITY) #check how many city names
res$newcity= str_to_upper(res$CITY) #create new column with upper case cities
res$newcity = recode(res$newcity,
"FUQUAY-VARINA" ="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA", "Fuquay Varina"="FUQUAY VARINA", "Fuquay-Varina" = "FUQUAY VARINA",
"RTP"= "RESEARCH TRIANGLE PARK",
"HOLLY SPRING"="HOLLY SPRINGS",
"MORRISVILE" = "MORRISVILLE",
) #recode Fuquay Varina, RTP, Holly Springs, and Morrisville
unique(res$newcity) #check to see if there are duplicates of cities
city_score_mean= group_by(res, newcity) %>% #groups the cities and finds the mean score of each city
summarize(average_score=mean(SCORE))
View(inspector_score)
View(inspector_score)
View(res)
View(res)
res[res$newcity="RALEIGH"]
res[res$newcity="RALEIGH",]
res[res$newcity=="RALEIGH",]
a=res[res$newcity=="RALEIGH",]
View(a)
View(a)
nrow(a)
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=sum(newcity))
unique(res$newcity)
newtab=table(res$newcity, useNA="ifany")
a=data.frame(rbind(newtab))
asd=data.frame(t(a[-1]))
View(asd)
View(asd)
View(asd)
View(asd)
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=sum(newcity))
city_inspections_count=group_by(res,newcity)%>%
summarize(inspection_count=nrow(newcity))
unique(res$newcity)
View(city_inspections_count)
View(city_inspections_count)
res %>%
count(newcity)
res %>%
count(newcity)
count=res %>%
count(newcity)
View(count)
View(count)
count=res %>%
count(inspection_count=newcity)
View(count)
View(count)
count=res %>%
count(newcity)
View(count)
View(count)
count=res %>%
count(City=newcity)
View(count)
View(count)
colnames(count)[2]="bruh"
View(count)
View(count)
city_inspections_count=res %>%
city_inspections_count(City=newcity)
colnames(city_inspections_count)[2]="bruh"
View(city_inspections_count)
View(city_inspections_count)
city_count=res %>%
city_inspections_count(City=newcity)
count=res %>%
count(City=newcity)
colnames(count)[2]="bruh"
what=res %>%
count(City=newcity)
colnames(count)[2]="bruh"
city_inspections_total=res %>%
count(City=newcity)
colnames(city_inspections_total)[2]="Number of inspections"
View(city_inspections_total)
View(city_inspections_total)
city_inspections_total=city_inspections_total[order(-city_inspections_total),]
city_inspections_total=city_inspections_total[order(-city_inspections_total$`Number of inspections`),]
View(city_inspections_total)
View(city_inspections_total)
View(city_inspections_total)
View(city_inspections_total)
View(res)
View(res)
View(city_score_mean)
View(city_score_mean)
View(city_score_mean)
View(city_score_mean)
city_inspections_total=res %>% #counts the number of inspections per city
count(newcity)
colnames(city_inspections_total)[2]="Number of inspections" #renames column name
city_inspections_total=city_inspections_total[order(-city_inspections_total$`Number of inspections`),] #sorts the data
newdata <- merge(city_inspections_total,
city_score_mean,
by.x="newcity")
View(newdata)
View(newdata)
View(city_inspections_total)
city_inspections_total=city_inspections_total[order(-city_inspections_total$`Number of inspections`),] #sorts the data
View(city_inspections_total)
View(city_inspections_total)
city_inspections_total=city_inspections_total[order(-city_inspections_total$`Number of inspections`),] #sorts the data
city_inspections_total <- merge(city_inspections_total,
city_score_mean,
by.x="newcity")
city_inspections_total=city_inspections_total[order(-city_inspections_total$`Number of inspections`),] #sorts the data
View(city_inspections_total)
View(city_inspections_total)
